<!DOCTYPE HTML>
<html lang="en-US">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Intelligent Design explained: Part 2 random search</title>
  
  <meta property="og:site_name" content="The Panda's Thumb" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="Intelligent Design explained: Part 2 random search" />
  <meta property="og:url" content="https://pandasthumb.org/archives/2006/06/intelligent-des-28.html" />
  <meta property="og:image" content="https://pandasthumb.org/media/roar.jpg" />
  <meta property="article:author" content="PvM" />
  <meta property="article:published_time" content="2006-06-21T19:43:35-07:00" />


  <link rel="stylesheet" type='text/css' href="/css/normalize.css" />
  <link rel="stylesheet" type='text/css' href="//fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,600,700,400italic,700italic|Source+Code+Pro:400,700" />
  <link rel="stylesheet" type='text/css' href="/css/gridism.css" />
  <link rel="stylesheet" type='text/css' href="/css/style.css" />
  <link rel="stylesheet" type='text/css' href="/css/github.css" />
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <script type="text/javascript" src="//code.jquery.com/jquery-3.1.1.min.js"></script>
  <script type="text/javascript" src="https://use.fontawesome.com/e6a24c6dde.js"></script>
  <script type="text/javascript" async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>  
  <script src="/js/local.js"></script>

</head>


<body class="wrap">
  <div class="grid page-fill"><div></div></div>
<header class="topbar">
  <div class="grid no-gutters">
    <div class="banner unit whole">
		  <img src="/media/banner.jpg" alt="The Panda's Thumb" />
    </div>
    <nav class="main-nav unit whole align-center">
      <ul>
<li><a href="/" title="Main Page"><i class="fa fa-home" aria-hidden="true"></i>&nbsp;Main</a></li>
<li><a href="/archives/index.html" title="Archives"><i class="fa fa-book" aria-hidden="true"></i>&nbsp;Archives</a></li>
<li><a href="/search.html" title="Searching Panda's Thumb"><i class="fa fa-book" aria-hidden="true"></i>&nbsp;Search</a></li>
</ul>

    </nav>
  </div>
</header>


    <section class="blog">
    <div class="grid">
      <div class="unit whole">
        <article class="blogpost">
  <header>
	  <h1><a href="/archives/2006/06/intelligent-des-28.html">Intelligent Design explained: Part 2 random search</a></h1>
	  <div class="byline"><i class="fa fa-user-circle" aria-hidden="true"></i> By PvM</div>
	  <div class="attrline"><i class="fa fa-clock-o" aria-hidden="true"></i>
	  <span><time pubdate="pubdate" datetime="2006-06-21T19:43:35-07:00">June 21, 2006 19:43 MST</time></span>
	  </div>
   </header>
  <h2 id="random-search-and-no-free-lunch">Random Search and No Free Lunch</h2>

<p>In his book “No Free Lunch”, Dembski argues that, based upon the <a href="http://en.wikipedia.org/wiki/No-Free-Lunch_theorems">No Free Lunch Theorems</a>,  finding an optimal solution via “random search” is virtually impossible because no evolutionary algorithm is superior to random search. And while various authors have shown the many problems with Dembski’s arguments, I intend to focus on a relatively small but devastating aspect of the <a href="http://en.wikipedia.org/wiki/No-Free-Lunch_theorems">No Free Lunch Theorems</a>.</p>

<p>First I will explain what the <a href="http://en.wikipedia.org/wiki/No-Free-Lunch_theorems">No Free Lunch Theorems</a> are all about, subsequently I will show how Dembski uses the <a href="http://en.wikipedia.org/wiki/No-Free-Lunch_theorems">No Free Lunch Theorems</a> and finally I will show that the <a href="http://en.wikipedia.org/wiki/No-Free-Lunch_theorems">No Free Lunch Theorems</a> show how a random search, perhaps counterintuitively, is actually quite effective.</p>

<h2 id="no-free-lunch-nfl-theorems">No Free Lunch (NFL) Theorems</h2>

<p>The <a href="http://en.wikipedia.org/wiki/No-Free-Lunch_theorems">No Free Lunch Theorems</a>, after which Dembski named his book, are based on a set of papers  by Wolpert and MacReady which basically state that:</p>

<blockquote>
  <p>”[…] all algorithms that search for an extremum of a cost function perform exactly the same, when averaged over all possible cost functions.” (Wolpert and Macready, 1995)</p>
</blockquote>

<h2 id="dembski-and-the-nfl">Dembski and the NFL</h2>

<p>Dembski argued, based on the <a href="http://en.wikipedia.org/wiki/No-Free-Lunch_theorems">No Free Lunch Theorems</a>, that evolutionary algorithms could not perform better than a random search.</p>

<p>Dembski wrote:</p>

<blockquote>
  <p>It’s against this backdrop of displacement that I treat the No Free Lunch theorems. These theorems say that when averaged across all fitness functions of a given class (each fitness function being an item of information that constrains an otherwise unconstrained search), no evolutionary algorithm is superior to blind or random search.</p>
</blockquote>

<p>and</p>

<p>Dembski wrote:</p>

<blockquote>
  <p>In general, arbitrary, unconstrained, maximal classes of fitness functions each seem to have a No Free Lunch theorem for which evolutionary algorithms cannot, on average, outperform blind search.</p>
</blockquote>

<p><strong>Source</strong>: Dembski <a href="http://acs.ucsd.edu/~idea/dembskiorr.htm">Evolution’s Logic of Credulity: An Unfettered Response to Allen Orr</a></p>

<p>While Dembski’s treatment of the “No Free Lunch” theorems was, according to mathematician David Wolpert, mostly <a href="http://www.talkreason.org/articles/jello.cfm">written in Jello</a>, it is still interesting to pursue some of Dembski’s claims. As I will show, not only do the No Free Lunch theorems fail to support Dembski’s thesis, but in fact the No Free Lunch theorems show that such optimization is child’s play.</p>

<p>The question really becomes: Is it really that hard to find an optimal solution using random search under the assumptions of the No Free Lunch Theorems?</p>

<p>The answer may be a surprise to many and it is ‘not really’.</p>

<h2 id="random-search">Random Search</h2>

<p>Finding the optimum value may be  hard but finding a solution which is almost as good, is actually reasonably simple. And since evolution does not necessarily search for the best solution, it quickly becomes clear that Dembski’s ‘No Free Lunch” may have little relevance to evolutionary theory.</p>

<p>In 2002, on the ISCID boards, Erik provided the following calculations:</p>

<p>Erik wrote:</p>

<blockquote>
  <p>Ironically, even if we grant that the prior over the set of all cost functions is uniform, the NFL theorem does not say that optimization is very difficult. <strong>It actually says that, when the prior is uniform, optimization is child’s play!</strong> I mean that almost literally. Almost any strategy no matter how elaborate or crude will do. If the prior over the set of cost functions is uniform, then so is the prior over the set of cost values. That means that if we sample a point in the search space we are equally likely to get a low cost value as a high cost value. Suppose that there are Y possible cost values. Then the probability a sampled point will have one of the L lowest cost values is just</p>

  <p>r = L / Y,</p>

  <p>regardless of which strategy that was used to decide which point to sample. The probability s that at least one of N different sampled points will have a cost value among the L best is given by</p>

  <p>s = 1 - (1 - r)^N,</p>

  <p>again independently of the strategy used. Is that good or bad performance? The number of points required to achieve a given performance and confidence level is</p>

  <p>N = ln(1 - s) / ln(1 - r) ~ - ln(1 - s) / r.</p>

  <p>After sampling 298 points the probability that at least one of them is among the best 1% is 0.95. After 916 sampled points the same probability is 0.9999. If instead we want a point among the best 0.1% we need to sample 2994 points to find one with probability 0.95, or approximately 9206 points to find one with probability 0.9999. That kind of performance may not be satisfactory when the optimization must be done very fast in real-time under critical conditions, but it is good for most purposes. Certainly our universe would seem to be able to spare the time necessary to sample 9206 points.</p>

  <p>This is why Thomas English wrote</p>

  <blockquote>
    <p>“The maligned uniform distribution is actually benign. The probability of finding one of the better points with n evaluations does not depend on the size of the domain [7]. For instance, 916 evaluations uncover with 99.99% certainty a point that is better than 99% of the domain. What is remarkable about NFL and the uniform is not just that simple enumeration of points is optimal, but that it is highly effective.” (see below for a reference)</p>
  </blockquote>

  <p><strong>Source</strong>: English T. (1999) “Some Information Theoretic Results On Evolutionary Optimization”, Proceedings of the 1999 Congress on Evolutionary Computation: CEC99, pp. 788-795</p>

  <p>The inference is never better than the assumption of a uniform prior that it relies on, however. It would seem that in most non-trivial optimization problems the number of good points in the search space are not as frequent as the number of bad points, meaning that the corresponding cost functions are not drawn uniformly from the set of all possible cost functions.</p>
</blockquote>

<p>As Erik pointed out, as early as 1996, Tom English derived how relatively simple optimization really is:</p>

<p>Tom English wrote:</p>

<blockquote>
  <p>The obvious interpretation of “no free lunch” is that no optimizer is faster, in general, than any other. This  misses some very important aspects of the result, however. One might conclude that all of the optimizers  are slow, because none is faster than enumeration. And one might also conclude that the unavoidable slowness derives from the perverse difficulty of the uniform distribution of test functions. Both of these conclusions would be wrong. 
If the distribution of functions is uniform, the optimizer’s best-so-far value is the maximum of n realizations of a uniform random variable. The probability that all n values are in the lower q fraction of the codomain is p = q<sup>n</sup>. Exploring n = log2 p points makes the probability p that all values are in the lower q  fraction. Table 1 shows n for several values of q and p. 
It is astonishing that in 99.99% of trials a value better than 99.999% of those in the codomain is obtained with fewer than one million evaluations. This is an average over all functions, of course. It bears mention that one of them has only the worst codomain value in its range, and another has only the best  codomain value in its range.</p>
</blockquote>

<p>Thomas M. English  <strong>Evaluation of Evolutionary and Genetic Optimizers:  No Free Lunch</strong> Evolutionary Programming V: Proceedings of the Fifth Annual Conference on Evolutionary Programming, L. J. Fogel, P. J. Angeline, and T BÃ¤ck, Eds., pp. 163-169. Cambridge, Mass: MIT Press, 1996.</p>

<table>
  <tbody>
    <tr>
      <td><strong>Fraction</strong></td>
      <td><strong>Probability</strong></td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>0.01</strong></td>
      <td><strong>0.001</strong></td>
      <td><strong>0.0001</strong></td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>0.99</strong></td>
      <td>458</td>
      <td>678</td>
      <td>916</td>
    </tr>
    <tr>
      <td><strong>0.999</strong></td>
      <td>4603</td>
      <td>6904</td>
      <td>9206</td>
    </tr>
    <tr>
      <td><strong>0.9999</strong></td>
      <td>46049</td>
      <td>69074</td>
      <td>929099</td>
    </tr>
    <tr>
      <td><strong>0.99999</strong></td>
      <td>460515</td>
      <td>690772</td>
      <td>921029</td>
    </tr>
  </tbody>
</table>

<p>Tom English repeated these facts on a posting to PandasThumb</p>

<blockquote>
  <p>In 1996 I showed that NFL is a symptom of conservation of information in search. Repeating a quote of Dembski above:</p>

  <p>Dembski wrote:</p>

  <blockquote>
    <p>The upshot of these theorems is that evolutionary algorithms, far from being universal problem solvers, are in fact quite limited problem solvers that depend crucially on additional information not inherent in the algorithms before they are able to solve any interesting problems. This additional information needs to be carefully specified and fine-tuned, and such specification and fine-tuning is always thoroughly teleological.</p>
  </blockquote>

  <p>Under the theorems’ assumption of a uniform distribution of problems, an uninformed optimizer is optimal. To be 99.99% sure of getting a solution better than 99.999% of all candidate solutions, it suffices to draw a uniform sample of just 921,029 solutions. Optimization is a benign problem with rare instances that are hard. Dembski increases the incidence of difficult instances by stipulating “interesting problems.” At that point it is no longer clear which NFL theorems he believes apply. Incidentally, an optimizer cannot tune itself to the problem instance while solving it, but its parameters can be tuned to the problem distribution from run to run. It is possible to automate adaptation of an optimizer to the problem distribution without teleology.</p>
</blockquote>

<p><strong>Source</strong>: Tom English  <a href="http://www.pandasthumb.org/archives/2004/06icons_of_id_no.html#comment-3883">Pandasthumb Comment</a></p>

<p>I cannot emphasize strongly enough how wrong Dembski is in his comments on random search as these almost trivial calculations reveal. While Dembski is correct that finding <strong>the</strong> optimal solution may be extremely hard, finding a solution which is arbitrarily close to the solution is actually quite straightforward.</p>

<p>It should not come as a surprise that the “No Free Lunch Theorems” have more unfortunate surprises in store for Intelligent Design. More on that later…</p>

<h2 id="no-free-lunch-theorems">No Free Lunch Theorems</h2>

<ul>
  <li>Dembski, <a href="http://www.arn.org/docs/dembski/wd_nfl_intro.htm">Introduction to No Free Lunch</a></li>
  <li>Richard Wein, <a href="http://www.talkorigins.org/design/faqs/nfl/">Not a Free Lunch But a Box of Chocolates A critique of William Dembski’s book No Free Lunch</a></li>
  <li>Wesley Elsberry, <a href="http://www.antievolution.org/people/dembski_wa/nflbook.html">No Free Lunch, The Book</a></li>
  <li>English, <a href="http://citeseer.ist.psu.edu/english00optimization.html">Optimization Is Easy and Learning Is Hard In the Typical Function (2000)</a>, Proceedings of the 2000 Congress on Evolutionary Computation</li>
  <li><a href="http://talkreason.org/index.cfm?category=10">Critique of Intelligent Design</a> Talk Reason</li>
  <li><a href="http://www.cs.uwyo.edu/~wspears/yin-yang.html">Yin-Yang: No-Free-Lunch Theorems for Search</a> Discussion at the 1995 International Conference on Genetic Algorithms</li>
  <li><a href="http://www.no-free-lunch.org/">No Free Lunch Theorems</a></li>
  <li><a href="http://en.wikipedia.org/wiki/No-free-lunch_theorem">Wikipedia: No Free Lunch Theorem</a></li>
  <li><a href="http://evolutionary.martinsewell.com/nfl/">The No Free Lunch Theorems, Evolution and Evolutionary Algorithms</a></li>
  <li><a href="http://www.talkorigins.org/indexcc/CF/CF011.html">Index to Creationist Claims</a></li>
  <li>Wolpert and MacReady, <a href="http://citeseer.ist.psu.edu/wolpert96no.html">No Free Lunch Theorems for Optimization (1996)</a></li>
  <li>Wolpert and MacReady <a href="http://citeseer.ist.psu.edu/wolpert95no.html">No Free Lunch Theorems for Search (1995)</a></li>
  <li>Wolpert <a href="http://talkreason.org/articles/jello.cfm">William Dembski’s treatment of the
No Free Lunch theorems is written in jello</a></li>
  <li>Mark Toussaint <a href="http://homepages.inf.ed.ac.uk/mtoussai/publications/index.html">Homepage Publications</a></li>
  <li>Marc Toussaint, Christian Igel <a href="http://citeseer.ist.psu.edu/toussaint02neutrality.html">Neutrality: A Necessity for Self-Adaptation (2002)  </a>, 2002</li>
  <li>Toussaint, <a href="http://citeseer.ist.psu.edu/sieber91phd.html">PhD thesis  The evolution of genetic representations and modular adaptation</a></li>
  <li>Toussaint, <a href="http://citeseer.ist.psu.edu/toussaint03evolution.html">On the Evolution of Phenotypic Exploration Distributions (2003)</a></li>
</ul>

</article>

      </div>

   
<div id="disqus_thread" class="unit whole"></div>
<script>
var disqus_config = function () {
this.page.url = 'https://pandasthumb.org/archives/2006/06/intelligent-des-28.html';
this.page.identifier = '/archives/2006/06/intelligent-des-28';
};
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//the-pandas-thumb.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



    </div>
  </section>


  <footer>
<div class="grid">
<table bgcolor="204517">
<tr><td>&nbsp;&nbsp;&nbsp;&nbsp;</td><td>
  <div align="left">
    <p>&nbsp;&nbsp;To see earlier posts, select the Archives at the top of this page</p>
    <p></p>
  </div>
  <div id="RecentComments" class="dsq-widget">
      <div align="center">
       <h2>Recent Comments</h2>
       <P>
       To see the comment in context of the discussion click on the text that indicates how long ago the comment was posted, such as "2 hours ago".  Then wait for the post 
     and then the comments to load (may take many seconds).  The comment should have a yellow vertical bar next to the commenter avatar.
      <P>
    </div>
    <div>
     <script type="text/javascript" src="https://the-pandas-thumb.disqus.com/recent_comments_widget.js?num_items=99&hide_mods=0&hide_avatars=0&avatar_size=32&excerpt_length=100"></script>
    </div>
  </div>
</td></tr>
  <tr><td></td><td><div class="unit whole">
      <p>Copyright &copy; The Panda&#8217;s Thumb and original authors &mdash; Content provided under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons BY-NC-ND License 4.0</a>.</p>
    <p></p>
    </div>
    </td></tr>
  </table>
  </div>
  </footer>

  

</body>

</html>


