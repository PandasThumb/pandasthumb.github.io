---
layout: article
title: The architecture of complexity
date: '2008-09-18 17:25:06 -0700'
author: PvM
mt_id: 3958
blog_id: 2
post_id: 3958
basename: the_architectur
published: false
---


<!--more-->

THE ARCHITECTURE OF COMPLEXITY
HERBERT A. SIMON•
Professor of Administration, Carnegie Institute of Technology
(Read April 26, 1962)
A NUMBER of proposals have been advanced in recent years for the development of "general sys¬tems theory" which, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them.' We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Meta¬phor and analogy can be helpful, or they can be misleading. All depends on whether the simi¬larities the metaphor captures are significant or superficial.
It may not be entirely vain, however, to search for common properties 'among diverse kinds of complex systems. The ideas that go by the name of cybernetics constitute, if not a theory, at least a point of view that has been proving fruitful over a wide range of applications.' It has been useful to look at the behavior of adaptive systems in terms of the concepts of feedback and homeostasis,
* The ideas in this paper have been the topic of many conversations with my colleague, Allen Newell. George W. Corner suggested important improvements in biologi¬cal content as well as editorial form. I am also indebted, for valuable comments on the manuscript, to Richard H. Meier, John R. Platt, and Warren Weaver. Some of the conjectures about the nearly decomposable structure of the nucleus-atom-molecule hierarchy were checked against the available quantitative data by Andrew Schoene and William Wise. My work in this area has been supported by a Ford Foundation grant for research in organizations and a Carnegie Corporation grant for research on cognitive processes. To all of the above, my warm thanks, and the usual absolution.
See°especially the yearbooks of the Society for Gen¬eral Systems Research. Prominent among the exponents of general systems theory are L. von Bertalanffy, K. Boulding, R. W. Gerard, and J. G. Miller. For a more skeptical view-perhaps too skeptical in the light of the present discussion--see H. A. Simon and A. Newell, Models: their uses and limitations, in L. D. White, ed., The state of the social sciences, 66-83, Chicago, Univ. of Chicago Press, 1956.
2 N. Wiener, Cybernetics, New York, John Wiley & Sons, 1948. For an imaginative forerunner, see A. J. Lotka, Elements of mathematical biology, New York, Dover Publications, 1951, first published in 1924 as Ele¬ments of physical biology. and to analyze adaptiveness in terms of the theory of selective information.8 The ideas of feedback and information provide a frame of reference for viewing a wide range of situations, just as do the ideas of evolution, of relativism, of axiomatic method, and of operationalism.
In this paper I should like to report on some things we have been learning about particular kinds of complex systems encountered in the be¬havioral sciences. The developments I shall dis¬cuss arose in the context of specific phenomena, but the theoretical formulations themselves make little reference to details of structure. Instead they refer primarily to the complexity of the sys¬tems under view without specifying the exact content of that complexity. Because of their abstractness, the theories may have relevance-- application would be too strong a term--to other kinds of complex systems that are observed in the social, biological, and physical sciences.
In recounting these developments, I shall avoid technical detail, which can generally be found elsewhere. I shall describe each theory in the particular context in which it arose. Then, I shall cite some examples of complex systems, from areas of science other than the initial application, to which the theoretical framework appears rele¬vant. In doing so, I shall make reference to areas of knowledge where I am not expert--perhaps not even literate. I feel quite comfortable in doing so before the members of this society, representing as it does the whole span of the scientific and scholarly endeavor. Collectively you will have little difficulty, I am sure, in distinguishing in¬stances based on idle fancy or sheer ignorance from instances that cast some light on the ways in which complexity exhibits itself wherever it is found in nature. I shall leave to you the final judgment of relevance in your respective fields.
I shall not undertake a formal definition of
3 C. Shannon and W. Weaver, The mathematical theory of communication, Urbana, Univ. of Illinois Press, 1949; W. R. Ashby, Design for a brain, New York, John Wiley & Sons, 1952.
PROCEEDINGS OF THE AMERICAN PHILOSOPHICAL SOCIETY, VOL 106, NO. 6, DECEMBER, 1962 Reprint	Printed in U.S.A.
468	HERBERT A. SIMON	[PROC. AMER. PHIL. SOC.
"complex systems." 4 Roughly, by a complex system I mean one made up of a large number of parts that interact in a nonsimple way. In such systems, the whole is more than the sum of the parts, not in in ultimate, metaphysical sense, but in the important pragmatic sense that, given the properties of the parts and the laws of their inter¬action, it is not a trivial matter to infer the prop¬erties of the whole. In the face of complexity, an in-principle reductionist may be at the same time a pragmatic holist.4
The four sections that follow discuss four as¬pects of complexity. The first offers some com¬ments on the frequency with which complexity takes the form of hierarchy--the complex system being composed of subsystems that, in turn, have their own subsystems, and so on. The second section theorizes about the relation between the structure of a complex system and the time re¬quired for it to emerge through evolutionary proc¬esses : specifically, it argues that hierarchic systems will evolve far more quickly than non-hierarchic systems of comparable size. The third section explores the dynamic properties of hierarchically- organized systems, and shows how they can be decomposed into subsystems in order to analyze their behavior. The fourth section examines the relation between complex systems and their de¬scriptions.
Thus, the central theme that runs through my remarks is that complexity frequently takes the form of hierarchy, and that hierarchic systems have some common properties that are independent of their specific content. Hierarchy, I shall argue, is one of the central structural schemes that the architect of complexity uses.
W. Weaver, in: Science and complexity, American Scientist 36: 536, 1948, has distinguished two kinds of complexity, disorganized and organized. We shall be primarily concerned with organized complexity.
8 See also John R. Platt, Properties of large molecules that go beyond the properties of their chemical sub-groups, Jour. Theoret. Biol. 1: 342-358, 1961. Since the reduc¬tionism-holism issue is a major cause de guerre between scientists and humanists, perhaps we might even hope that peace could be negotiated between the two cultures along the lines of the compromise just suggested. As I go along, I shall have a little to say about complexity in the arts as well as in the natural sciences. I must empha¬size the pragmatism of my holism to distinguish it sharply from the position taken by W. M. Elsasser in The physi¬cal foundation of biology, New York, Pergamon Press, 1958.
HIERARCHIC SYSTEMS
By a hierarchic system, or hierarchy, I mean a system that is composed of interrelated sub¬systems, each of the latter being, in turn, hier¬archic in structure until we reach some lowest level of elementary subsystem. In most systems in nature, it is somewhat arbitrary as to where we leave off the partitioning, and what subsystems we take as elementary. Physics makes much use of the concept of "elementary particle" although particles have a disconcerting tendency not to remain elementary very long. Only a couple of generations ago, the atoms themselves were elementary particles; today, to the nuclear physi¬cist they are complex systems. For certain pur¬poses of astronomy, whole stars, or even galaxies, can be regarded as elementary subsystems. In one kind of biological research, a cell may be treated as an elementary subsystem; in another, a protein molecule ; in still another, an amino acid residue.
Just why a scientist has a right to treat as ele¬mentary a subsystem that is in fact exceedingly complex is one of the questions we shall take up. For the moment, we shall accept the fact that scientists do this all the time, and that if they are careful scientists they usually get away with it.
Etymologically, the word "hierarchy" has had a narrower meaning than I am giving it here. The term has generally been used to refer to a complex system in which each of the subsystems is subordinated by an authority relation to the system it belongs to. More exactly, in a hier¬archic formal organization, each system consists of a "boss" and a set of subordinate subsystems. Each of the subsystems has a "boss" who is the immediate subordinate of the boss of the system. We shall want to consider systems in which the relations among subsystems are more complex than in the formal organizational hierarchy just described. We shall want to include systems in which there is no relation of subordination among subsystems. (In fact, even in human organiza¬tions, the formal hierarchy exists only on paper; the real flesh-and-blood organization has many inter-part relations other than the lines of formal authority.) For lack of a better term, I shall use hierarchy in the broader sense introduced in the previous paragraphs, to refer to all complex sys¬tems analyzable into successive sets of subsystems, and speak of "formal hierarchy" when I want to refer to the more specialized concept .°
• The mathematical term "partitioning" will not do for what I call here a hierarchy; for the set of subsystems,
-•••••••••••••••
VOL. 106, NO. 6, 1962]	THE ARCHITECTURE OF COMPLEXITY	469
SOCIAL SYSTEMS
I have already given an example of one kind of hierarchy that is frequently encountered in the social sciences : a formal organization. Business firms, governments, universities all have a clearly visible parts-within-parts structure. But formal organizations are not the only, or even the most common, kind of social hierarchy. Almost all societies have elementary units called families, which may be grouped into villages or tribes, and these into larger groupings, and so on. If we make a chart of social interactions, of who talks to whom, the clusters of dense interaction in the chart will identify a rather well-defined hierarchic structure. The groupings in this structure may be defined operationally by some measure of fre¬quency of interaction in this sociometric matrix.
BIOLOGICAL AND PHYSICAL SYSTEMS
The hierarchical structure of biological systems is a familiar fact. Taking the cell as the building block, we find cells organized into tissues, tissues into organs, organs into systems. Moving down¬ward from the cell, well-defined subsystems--for example, nucleus, cell membrane, microsomes, mitochondria, and so on--have been identified in animal cells.
The hierarchic structure of many physical sys¬tems is equally clear-cut. I have already men¬tioned the two main series. At the microscopic level we have elementary particles, atoms, mole¬cules, macromolecules. At the macroscopic level we have satellite systems, planetary systems, gal¬axies. Matter is distributed throughout space in a strikingly non-uniform fashion. The most nearly random distributions we find, gases, are not ran¬dom distributions of elementary particles but random distributions of complex systems, i.e. molecules.
A considerable range of structural types is sub¬sumed under the term hierarchy as I have defined it. By this definition, a diamond is hierarchic, for it is a crystal structure of carbon atoms that can be further decomposed into protons, neutrons, and electrons. However, it is a very "flat" hier¬archy, in which the number of first-order sub¬systems belonging to the crystal can be indefinitely large. A volume of molecular gas is a fiat hier¬archy in the same sense. In ordinary usage, we
and the successive subsets in each of these dennes the partitioning, independently of any systems of relations among the subsets. By hierarchy I mean the partitioning in conjunction with the relations that hold among its parts.
tend to reserve the word hierarchy for a system that Is divided into a small or moderate number of subsystems, each of which may be further sub¬divided. Hence, we do not ordinarily think of or refer to a diamond or a gas as a hierarchic struc¬ture. Similarly, a linear polymer is simply a chain, which may be very long, of identical sub¬parts, the monomers. At the molecular level it is a very flat hierarchy.
In discussing formal organizations, the number of subordinates who report directly to a single boss is called his span of control. I will speak analo¬gously of the span of a system, by which I shall mean the number of subsystems into which it is partitioned. Thus, a hierarchic system is fiat at a given level if it has a wide span at that level. A diamond has a wide span at the crystal level, but not at the next level down, the molecular level.
In most of our theory construction in the fol¬lowing sections we shall focus our attention on hierarchies of moderate span, but from time to time I shall comment on the extent to which the theories might or might not be expected to apply to very flat hierarchies.
There is one important difference between the physical and biological hierarchies, on the one hand, and social hierarchies, on the other. Most physical and biological hierarchies are described in spatial terms. We detect the organelles in a cell in the way we detect the raisins in a cake-- they are "visibly" differentiated substructures lo¬calized spatially in the larger structure. On the other hand, we propose to identify social hier¬archies not by observing who lives close to whom but by observing who interacts with whom. These two points of view can be reconciled by defining hierarchy in terms of intensity of interaction, but observing that in most biological and physical sys¬tems relatively intense interaction implies rela¬tive spatial propinquity. One of the interesting characteristics of nerve cells and telephone wires is that they permit very specific strong interactions at great distances. To the extent that interactions are channeled through specialized communications and transportation systems, spatial propinquity becomes less determinative of structure.
SYMBOLIC SYSTEMS
One very important class of systems has been omitted from my examples thus far : systems of human symbolic production. A book is a hier¬archy in the sense in which I am using that term. It is generally divided into chapters, the chapters
470	HERBERT A. SIMON	IPROC. AYER. PHIL. SOC.
into sections, the sections into paragraphs, the paragraphs into sentences, the sentences into clauses and phrases, the clauses and phrases into words. We may take the words as our elementary units, or further subdivide them, as the linguist often does, into smaller units. If the book is nar¬rative in character, it may divide into "episodes" instead of sections, but divisions there will be.
The hierarchic structure of music, based on such units as movements, parts, themes, phrases, is well known. The hierarchic structure of products of the pictorial arts is more difficult to characterize, but I shall have something to say about it later.
THE EVOLUTION OF COMPLEX SYSTEMS
Let me introduce the topic of evolution with a parable. There once were two watchmakers, named Hora and Tempus, who manufactured very fine watches. Both of them were highly regarded, and the phones in their workshops rang frequently --new customers were constantly calling them. However, Hora prospered, while Tempus became poorer and poorer and finally lost his shop. What was the reason ?
The watches the men made consisted of about 1,000 parts each. Tempus had so constructed his that if he had one partly assembled and had to put it down--to answer the phone say--it immediately fell to pieces and had to be reassembled from the elements. The better the customers liked his watches, the more they phoned him, the more diffi¬cult it became for him to find enough uninterrupted time to finish a watch.
The watches that Hora made were no less com¬plex than those of Tempus. But he had designed them so that he could put together subassemblies of about ten elements each. Ten of these subas¬semblies, again, could be put together into a larger subassembly ; and a system of ten of the latter sub¬assemblies constituted the whole watch. Hence, when Hora had to put down a partly assembled watch in order to answer the phone, he lost only a small part of his work, and he assembled his watches in only a fraction of the man-hours it took Tempus.
It is rather easy to make a quantitative analysis of the relative difficulty of the tasks of Tempus and Hora : Suppose the probability that an inter¬ruption will occur while a part is being added to an incomplete assembly is p. Then the probabil¬ity that Tempus can complete a watch he has started without interruption is (1-p)1001--a very small number unless p is .001 or less. Each in¬terruption will cost, on the average, the time to as¬ semble 1/p parts (the expected number assembled before interruption). On the other hand, Hora has to complete one hundred eleven sub-assemblies of ten parts each. The probability that he will not be interrupted while completing any one of these is (1-p)", and each interruption will cost only about the time required to assemble five parts!
Now if p is about .01--that is, there is one chance in a hundred that either watchmaker will be interrupted while adding any one part to an as¬sembly--then a straightforward calculation shows that it will take Tempus, on the average, about four thousand times as long to assemble a watch as Hora.
We arrive at the estimate as follows :
1.	Hora must make 111 times as many complete assemblies per watch as Ternpus; but,
2.	Tempus will lose on the average 20 times as much work for each interrupted assembly as Hora [100 parts, on the average, as against 5] ; and,
3. Tempus will complete an assembly only = 44 nly 44 times per million attempts (. i
x 10-11), while Hora will complete nine out of ten (.9910 = 9 x 10-1). Hence Tempus will have to make 20,000 as, many attempts per completed assembly as Hora. (9 X 10-1)/(44 x 10-4) = 2x 10•. Multiplying these three ratios, we get :
1/111 x 100/5 x .9910/.991000
= 1/111 x 20 x 20,000 •--• 4,000.
7 The speculations on speed of evolution were first sug¬gested by H. Jacobson's application of information theory to estimating the time required for biological evolution. See his paper, Information, reproduction, and the origin of life, in American Scientist 43: 119-127, January, 1955. From thermodynamic considerations it is possible to esti¬mate the amount of increase in entropy that occurs when a complex system decomposes into its elements. (See, for example, R. B. Setlow and E. C. Pollard, Molecular biophysics, 63-65, Reading, Mass., Addison-Wesley Pub¬lishing Co., 1962, and references cited there.) But entropy h the logarithm of a probability, hence information, the negative of entropy, can be interpreted as the logarithm of the reciprocal of the probability-the "improbability," so to speak. The essential idea in Jacobson's model is that the expected time required for the system to reach a particular state is inversely proportional to the proba¬bility of the state--hence increases exponentially with the amount of information (negentropy) of the state.
Following this line of argument, but not introducing the notion of levels and stable subassemblies, Jacobson arrived at estimates of the time required for evolution so large as to make the event rather improbable. Our analy¬sis, carried through in the same way, but with attention to the stable intermediate forms, produces very much smaller estimates.
VOL. 106, NO. 6, 19621	THE ARCHITECTURE OF COMPLEXITY	471
BIOLOGICAL EVOLUTION
What lessons can we draw from our parable for biological evolution? Let us interpret a par¬tially completed subassembly of k elementary parts as the coexistence of k parts in a small volume-- ignoring their relative orientations. The model assumes that parts are entering the volume at a constant rate, but that there is a constant prob¬ability, p, that the part will be dispersed before another is added, unless the assembly reaches a stable state. These assumptions are not particu¬larly realistic. They undoubtedly underestimate the decrease in probability of achieving the assem¬bly with increase in the size of the assembly. Hence the assumptions understate--probably by a large factor--the relative advantage of a hier¬archic structure.
Although we cannot, therefore, take the nu¬merical estimate seriously the lesson for biological evolution is quite clear and direct. The time re¬quired for the evolution of a complex form from simple elements depends critically on the numbers and distribution of potential intermediate stable forms. In particular, if there exists a hierarchy of potential stable "subassemblies," with about the same span, s, at each level of the hierarchy, then the time required for a subassembly can be ex¬pected to be about the same at each level--that is proportional to 1/(1-p)'. The time required for the assembly of a systen. of n elements will be proportional to log, n, that is, to the number of levels in the system. One would say--with more illustrative than literal intent--that the time re¬quired for the evolution of multi-celled organisms from single-celled organisms might be of the same order of magnitude as the time required for the evolution of single-celled organisms from macro¬molecules. The same argument could be applied to the evolution of proteins from amino acids, of molecules from atoms, of atoms from elementary particles.
A whole host of objections to this oversimplified scheme will occur, I am sure, to every working biologist, chemist, and physicist. Before turning to matters I know more about, I shall mention three of these problems, leaving the rest to the attention of the specialists.
First, in spite of the overtones of the watch¬maker parable, the theory assumes no teleological mechanism. The complex forms can arise from the simple ones by purely random processes. (I shall propose another model in a moment that shows this clearly.) Direction is provided to the scheme by the stability of the complex forms, once these come into existence. But thii is nothing more than survival of the fittest--i.e., of the stable.
Second, not all large systems appear hierarchi¬cal. For example, most polymers--e.g., nylon-- are simply linear chains of large numbers of identical components, the monomers. However, for present purposes we can simply regard such a structure as a hierarchy with a span of one--the limiting case. For a chain of any length repre¬sents a state of relative equilibrium.'
Third, the evolution of complex systems from simple elements implies nothing, one way or the other, about the change in entropy of the entire system. If the process absorbs free energy, the complex system will have a smaller entropy than the elements ; if it releases free energy, the oppo¬site will be true. The former alternative is the one that holds for most biological systems, and the net inflow of free energy has to be supplied from the sun or some other source if the second law of thermodynamics is not to be violated. For the evolutionary process we are describing, the equilibria of the intermediate states need have only local and not global stability, and they may be stable only in the steady state--that is, as long as there is an external source of free energy that may be drawn upon,
Because organisms are not energetically closed systems, there is no way to deduce the direction, much less the rate, of evolution from classical thermodynamic considerations. All estimates in¬dicate that the amount of entropy, measured in physical units, involved in the formation of a one- celled biological organism is trivially small--about --10-11 cal/degree." The "improbability" of evo¬lution has nothing to do with this quantity of entropy, which is produced by every bacterial cell every generation. The irrelevance of quantity of
•	There is a well-developed theory of polymer size, based on models of random assembly. See for example P. J. Flory, Principles of polymer chemistry, ch. 8, Ithaca, Cornell Univ. Press, 1953. Since all subassem- blies. in the polymerization theory are stable, limitation of molecular growth depends on "poisoning" of terminal groups by impurities or formation of cycles rather than upon disruption of partially-formed chains.
•	This point has been made many times before, but it cannot be emphasized too strongly. For further discus¬sion, see Setlow and Pollard, op. cit., 49-64; E. Schro¬dinger, What is life! Cambridge Univ. Press, 1945; and H. Linschitz, The information content of a bacterial cell, in H. Questler, ed., Information theory in biology, 251¬262, Urbana, Univ. of Illinois Press, 1953.
30 See Linschitz, op. cit. This quantity, 10-11 cal/de¬gree, corresponds to obout 1013 bits of information.
472	HERBERT A. SIMON	[PROC. AMER. PHIL. SOC.
information, in this sense, to speed of evolution can also be seen from the fact that exactly as much information is required to "copy" a cell through the reproductive process as to produce the first cell through evolution.
The effect of the existence of stable intermediate forms exercises a powerful effect on the evolution of complex forms that may be likened to the dra¬matic effect of catalysts upon reaction rates and steady state distribution of reaction products in open systems." In neither case does the entropy change provide us with a guide to system behavior.
PROBLEM SOLVING AS NATURAL SELECTION
Let us turn now to some phenomena that have no obvious connection with biological evolution : human problem-solving processes. Consider, for example, the task of discovering the proof for a difficult theorem. The process can be--and often has been--described as a search through a maze. Starting with the axioms and previously proved theorems, various transformations allowed by the rules of the mathematical systems are attempted, to obtain new expressions. These are modified in turn until, with persistence and good fortune, a sequence or path of transformations is discovered that leads to the goal.
The process usually involves a great deal of trial and error. Various paths are tried ; some are abandoned, others are pushed further. Before a solution is found, a great many paths of the maze may be explored. The more difficult and novel the problem, the greater is likely to be the amount of trial and error required to find a solution. At the same time, the trial and error is not com¬pletely random or blind ; it is, in fact, rather highly selective. The new expressions that are obtained by transforming given ones are examined to see whether they represent progress toward the goal. Indications of progress spur further search in the same direction ; lack of progress signals the abandonment of a line of search. Problem solving requires selective trial and error."
" See H. Kacser, Some physico-chemical aspects of biological organization, Appendix, pp. 191-249 in C. H. WaddingtOn, The strategy of the genes, London, George Allen & Unwin, 1957.
12 See A. Newell, J. C. Shaw, and H. A. Simon, Empirical explorations of the logic theory machine, Pro¬ceedings of the 1957 Western Joint Computer Conference, February, 1957, New York: Institute of Radio Engi¬neers; Chess-playing programs and the problem of com¬plexity, IBM Journal of Research and Development. 2: 320-335, October, 1958; and for a similar view of prob¬lem solving, W. R. Ashby, Design for an intelligence
A little reflection reveals that cues signaling progress play the same role in the problem-solving process that stable intermediate forms play in the biological evolutionary process. In fact, we can take over the watchmaker parable and apply it also to problem solving. In problem solving, a partial result that represents recognizable progress toward the goal plays the role of a stable sub¬assembly.
Suppose that the task is to open a safe whose lock has ten dials, each with one hundred possible settings, numbered from 0 to 99. How long will it take to open the safe by a blind trial-and-error search for the correct setting? Since there are 10010 possible settings, we may expect to examine about one-half of these, on the average, before finding the correct one--that is, fifty billion billion settings. Suppose, however, that the safe is de¬fective, so that a click can be heard when any one dial is turned to the correct setting. Now each dial can be adjusted independently, and does not need to be touched again while the others are being set. The total number of settings that has to be tried is only 10 x 50, or five hundred. The task of opening the safe has been altered, by the cues the clicks provide, from a practically impossible one to a trivial one."
A considerable amount has been learned in the past five years about the nature of the mazes that represent common human problem-solving tasks-- proving theorems, solving puzzles, playing chess, making investments, balancing assembly lines, to mention a few. All that we have learned about these mazes points to the same conclusion : that human problem solving, from the most blundering to the most insightful, involves nothing more than varying mixtures of trial and error and selectivity. The selectivity derives from various rules of
amplifier, 215-233 in C. E. Shannon and J. McCarthy, Automata studies, Princeton, Princeton Univ. Press, 1956.
"The clicking safe example was supplied by D. P. Simon. Ashby, op. cit., 230, has called the selectivity involved in situations of this kind "selection by compo¬nents." The even greater reduction in time produced by hierarchiution in the clicking safe example, as compared with the watchmaker's metaphor, is due to the fact that a random search for the correct combination is involved in the former case, while in the latter the parts come to¬gether in the right order. It is not clear which of these metaphors provides the better model for biological evo¬lution, but we may be sure that the watchmaker's meta¬phor gives an exceedingly conservative estimate of the savings due to hierarchization. The safe may give an excessively high estimate because it assumes all possible arrangements of the elements to be equally probable.
VOL. 106, NO. 6, 1962]	THE ARCHITECTURE OF COMPLEXITY	473
thumb, or heuristics, that suggest which paths should be tried first and which leads are promising. We do not need to postulate processes more sophisticated than those involved in organic evo¬lution to explain how enormous problem mazes are cut down to quite reasonable size."
THE SOURCES OF SELECTIVITY
When we examine the sources from which the problem-solving system, or the evolving system, as the case may be, derives its selectivity, we dis¬cover that selectivity can always be equated with some kind of feedback of information from the environment.
Let us consider the case of problem solving first. There are two basic kinds of selectivity. One we have already noted : various paths are tried out, the consequences of following them are noted, and this information is used to guide further search. In the same way, in organic evolution, various complexes come into being, at least evanescently, and those that are stable provide new building blocks for further construction. It is this-infor¬mation about stable configurations, and not free energy or negentropy from the sun, that guides the process of evolution and provides the selectivity that is essential to account for its rapidity.
The second source of selectivity in problem solving is previous experience. We see this par¬ticularly clearly when the problem to be solved is similar to one that has been solved before. Then, by simply trying again the paths that led to the earlier solution, or their analogues, trial-and-error search is greatly reduced or altogether eliminated.
What corresponds to this latter kind of informa¬tion in organic evolution? The closest analogue is reproduction. Once we reach the level of self- reproducing systems, a complex system, when it has once been achieved, can be multiplied indefi¬nitely. Reproduction in fact allows the inheritance of acquired characteristics, but at the level of genetic material, of course ; i.e., only characteristics acquired by the genes can be inherited. We shall return to the topic of reproduction in the final section of this paper.
ON EMPIRES AND EMPIRE-BUILDING
We have not exhausted the categories of com¬plex systems to which the watchmaker argument can reasonably be applied. Philip assembled his
14 A. Nen•11 and H. A. Simon, Computer simulation of human thinking, Science 134: 2011-2017, December 22, 1961.
Macedonian empire and gave it to his son, to be later combined with the Persian subassembly and others into Alexander's greater system. On Alex¬ander's death, his empire did not crumble to dust, but fragmented into some of the major subsystems that had composed it.
The watchmaker argument implies that if one would be Alexander, one should be born into a world where large stable political systems already exist. Where this condition was not fulfilled, as on the Scythian and Indian frontiers, Alexander found empire building a slippery business. So too, T. E. Lawrence's organizing of the Arabian revolt against the Turks was limited by the charac¬ter of his largest stable building blocks, the sepa¬rate, suspicious desert tribes.
The profession of history places a greater value upon the validated particular fact than upon ten¬dentious generalization. I shall not elaborate upon my fancy, therefore, but will leave it to historians to decide whether anything can be learned for the interpretation of history from an abstract theory of hierarchic complex systems.
CONCLUSION : THE EVOLUTIONARY EXPLANATION
OF HIERARCHY
We have shown thus far that complex systems will evolve from simple systems much more rapidly if there are stable intermediate forms than if there are not. The resulting complex forms in the for¬mer case will be hierarchic. We have only to turn the argument around to explain the observed pre¬dominance of hierarchies among the complex sys¬tems nature presents to us. Among possible complex forms, hierarchies are the ones that have the time to evolve. The hypothesis that complex¬ity will be hierarchic makes no distinction among very flat hierarchies, like crystals, and tissues, and polymers, and the intermediate forms. Indeed, in the complex systems we encounter in nature, ex¬amples of both forms are prominent. A more complete theory than the one we have developed here would presumably have something to say about the determinants of width of span in these systems.
NEARLY DECOMPOSABLE SYSTEMS
In hierarchic systems, we can distinguish be¬tween the interactions among subsystems, on the one hand, and the interactions within subsystems --i.e., among the parts of those subsystems--on the other. The interactions at the different levels may be, and often will be, of different orders of
474	HERBERT A. SIMON	[PkOC. AMEN. PHIL. SOC.
CI C2 C3
100 --	2
2
Al --
A2 100 -- 100
A3 -- 100
Al A2 A3 I B1 B2
Bl	2 1 --
B2 --	1 2
-- 100
100 --
2 1 --
1	2
-- 100 --
100 -- 100
C1 --
C2 -- --
2 --
1 -
C3 -- -- 2 100
FIG. 1. A hypothetical nearly-decomposable system. In terms of the heat-exchange example of the text, Al, A2, and A3 may be interpreted as cubicles in one room, B1 and B2 as cubicles in a second room, and Cl, C2, and C3 as cubicles in a third. The matrix en¬tries then are the heat diffusion coefficients between cubicles.
Al	Cl
B1
A2	C2
B2
AS	C3
magnitude. In a formal organization there will generally be more interaction, on the average, be¬tween two employees who are members of the same department than between two employees from different departments. In organic sub¬stances, intermolecular forces will generally be weaker than molecular forces, and molecular forces than nuclear forces.
In a rare gas, the intermolecular forces will be negligible compared to those binding the molecules --we can treat the individual particles, for many purposes, as if they were independent of each other. We can describe such a system as decom¬posable into the subsystems comprised of the indi¬vidual particles. As the gas becomes denser, mo¬lecular interactions become more significant. But over some range, we can treat the decomposable case as a limit, and as a first approximation. We can use a theory of perfect gases, for example, to describe approximately the behavior of actual gases if they are not too dense. As a second ap¬proximation, we may move to a theory of nearly decomposable systems, in which the interactions among the subsystems are weak, but not negligible.
At least some kinds of hierarchic systems can be approximated successfully as nearly decomposable systems. The main theoretical findings from the approach can be summed up in two propositions : (a) in a nearly decomposable system, the short- run behavior of each of the component subsystems is approximately independent of the short-run be¬havior of the other components ; (b) in the long run, the behavior of any one of the components depends in only an aggregate way on the behavior of the other components.
Let me provide a very concrete simple example of a nearly decomposable system." Consider a building whose outside walls provide perfect thermal insulation from the environment. We shall take these walls as the boundary of our sys¬tem. The building is divided into a large number of rooms, the walls between them being good, but not perfect, insulators. The walls between rooms are the boundaries of our major subsystems. Each room is divided by partitions into a number of cubicles, but the partitions are poor insulators. A thermometer hangs in each cubicle. Suppose that at the time of our first observation of the sys¬tem there is a wide variation in temperature from cubicle to cubicle and from room to room--the various cubicles within the building are in a state of thermal disequilibrium. When we take new temperature readings several hours later, what shall we find? There will be very little variation in temperature among the cubicles within each single room, but there may still be large tempera¬ture variations among rooms. When we take readings again several days later, we find an al¬most uniform temperature throughout the build¬ing; the temperature differences among rooms have virtually disappeared.
We can describe the process of equilibration formally by setting up the usual equations of heat flow. The equations can be represented by the matrix of their coefficients, r,j, where rij is the rate at which heat flows from the ith cubicle to the jth cubicle per degree difference in their temperatures. If cubicles i and j do not have a common wall, ro will be zero. If cubicles i and j have a common wall, and are in the same room, ro will be large. If cubicles i and j are separated by the wall of a
15 This discussion of near-decomposability is based upon H. A. Simon and A. Ando, Aggregation of variables in dynamic systems, Economctrica 29: 111-138, April, 1961. The example is drawn from the same source, 117-118. The theory has been further developed and applied to a variety of economic and political phenomena by Ando and F. M. Fisher. See F. M. Fisher, On the cost of approximate specification in simultaneous equation estimation, Econometrica 29: 139-170, April, 1961, and F. M. Fisher and A. Ando, Two theorems on Ceteris Paribus in the analysis of dynamic systems, American Political Science Review 61: 103-113, March, 1962.
VOL. 106, NO. 6, 19621	THE ARCHITECTURE OF COMPLEXITY	475
room, rd.; will be nonzero but small. Hence, by grouping all the cubicles together that are in the same room, we can arrange the matrix of coeffi¬cients so that all its large elements lie inside a string of square submatrices along the main di¬agonal. All the elements outside these diagonal squares will be either zero or small (see figure 1). We may take some small number, c, as the upper bound of the extradiagonal elements. We shall call a matrix having these properties a nearly decomposable matrix.
Now it has been proved that a dynamic system that can be described by a nearly decomposable matrix has the properties, stated above, of a nearly decomposable system. In our simple example of heat flow this means that in the short run each room will reach an equilibrium temperature (an average of the initial temperatures of its "offices) nearly independently of the others; and that each room will remain approximately in a state of equi¬librium over the longer period during which an over-all temperature equilibrium is being estab¬lished throughout the building. After the intra¬room short-run equilibria have been reached, a single thermometer in each room will be adequate to describe the dynamic behavior of the entire system--separate thermometers in each cubicle will be superfluous.
NEAR DECOMPOSABILITY OF SOCIAL SYSTEMS
As a glance at figure 1 shows, near decomposa¬bility is a rather strong property for a matrix to possess, and the matrices that have this property will describe very special dynamic systems--van¬ishingly few systems out of all those that are thinkable. How few they will be depends, of course, on how good an approximation we insist upon. If we demand that epsilon be very small, correspondingly few dynamic systems will fit the definition. But we have already seen that in the natural world nearly decomposable systems are far from rare: On the contrary, systems in which each variable is linked with almost equal strength with almost all other parts of the system are far. rarer and less typical.
In economic dynamics, the main variables are the prices and quantities of commodities. It is empirically true that the price of any given com¬modity and the rate at which it is exchanged de¬pend to a significant extent only on the prices and quantities of a few other commodities, together with a few other aggregate magnitudes, like the average price level or some over-all measure of economic activity. The large linkage coefficients are associated, in general, with the main flows of raw materials and semi-finished products within and Between industries. An input-output matrix of the economy, giving the magnitudes of these flows, reveals the nearly decomposable structure of the system--with one qualification. There is a consumption subsystem of the economy that is linked strongly to variables in most of the other subsystems. Hence, we have to modify our no¬tions of decomposability slightly to accommodate the special role of the consumption subsystem in our analysis of the dynamic behavior of the economy.
In the dynamics of social systems, where mem¬bers of a system communicate with and influence other members, near decomposability is generally very prominent. This is most obvious in formal organizations, where the formal authority rela¬tion connects each member of the organization with one immediate superior and with a small number of subordinates. Of course many com¬munications in organizations follow other channels than the lines of formal authority. But most of these channels lead from any particular individual to a very limited number of his superiors, sub¬ordinates, and associates. Hence, departmental boundaries play very much the same role as the walls in our heat example.
PHYSICO-CHEMICAL SYSTEMS
In the complex systems familiar in biological chemistry, a similar structure is clearly visible. Take the atomic nuclei in such a system as the elementary parts of the system, and construct a matrix of bond strengths between elements. There will be matrix elements of quite different orders of magnitude. The largest will generally corre¬spond to the covalent bonds, the next to the ionic bonds, the third group to hydrogen bonds, still smaller linkages to van der Waals forces." If we select an epsilon just a little smaller than the mag¬nitude of a covalent bond, the system will de¬compose into subsystems--the constituent mole¬cules. The smaller linkages will correspond to the intermolecular bonds.
It is well known that high-energy, high-fre 
ie For a survey of the several classes of molecular and inter-molecular forces, and their dissociation energies see Setlow and Pollard, op. cit., chapter 6. The energies of typical covalent bonds are of the order of 80-100 k cal/mole, of the hydrogen bonds, 10 k cal/mole. Ionic bonds generally lie between these two levels, the bonds due to van der Waals forces are lower in energy.
476	HERBERT A. SIMON	frxoc. ALIEk. PHIL. SOC.
quency vibrations are associated with the smaller physical subsystems, low-frequency vibrations with the larger systems into which the subsystems are assembled. For example, the radiation fre¬quencies associated with molecular vibrations are much lower than those associated with the vibra¬tions of the planetary electrons of the atoms; the latter, in turn, are lower than those associated with nuclear processes." Molecular systems are nearly decomposable systems, the short-run 'dynamics relating to the internal structures of the subsys¬tems ; the long-run dynamics to the interactions of these subsystems.
A number of the important approximations em¬ployed in physics depend for their validity on the near-decomposability of the systems studied. The theory of the thermodynamics of irreversible proc¬esses, for example, requires the assumption of macroscopic disequilibrium but microscopic equi¬librium,1° exactly the situation described in our heat-exchange example. Similarly computations in quantum mechanics are often handled by treat¬ing weak interactions as producing perturbations on a system of strong interactions.
SOME OBSERVATIONS ON HIERARCHIC SPAN
To understand why the span of hierarchies is sometimes very broad--as in crystals--sometimes narrow, we need to examine more detail of the in¬teractions. In general, the critical consideration is the extent to which interaction between two (or a few) subsystems excludes interaction of these subsystems with the others. Let is examine first some physical examples.
Consider a gas of identical molecules, each of which can form covalent bonds, in certain ways, with others. Let us suppose that we can associate with each atom a specific number of bonds that it is capable of maintaining simultaneously. (This number is obviously related to the number we usu¬ally call its valence.) Now suppose that two atoms join, and that we can also associate with the com¬bination a specific number of external bonds it is capable of maintaining. If this number is the same
"Typical wave numbers for vibrations associated with various systems (the wave number is the reciprocal of wave length hence proportional to frequency) :
steel wire under tension-10' to 10' aria molecular rotations-100 to l02 cm-1
molecular vibrations-10' to 10' cm'
planetary electrons-104 to 10' cm -1
nuclear rotations-10' to 10'4 cnis
nuclear surface vibrations-1022 to 1022 cm-1.
22 S. R. de Groot, Thermodynamic: of irteversible proc¬esses, 11-12, New York, Interscience Publishers, 1951. as the number associated with the individual atoms, the bonding process can go on indefinitely--the atoms can form crystals or polymers of indefinite extent. If the number of bonds of which the composite is capable is less than the number as¬sociated with each of the parts, then the process of agglomeration must come to a halt.
We need only mention some elementary ex¬amples. Ordinary gases show no tendency to ag¬glomerate because the multiple bonding of atoms "uses up" their capacity to interact. While each oxygen atom has a valence of two, the 0. mole¬cules have a zero valence. Contrariwise, indefinite chains of single-bonded carbon atoms can be built up because a chain of any number of such atoms, each with two side groups, has a valence of ex¬actly two.
Now what happens if we have a system of ele¬ments that possess both strong and weak inter¬action capacities, and whose strong bonds are ex¬haustible through combination? Subsystems will form, until all the capacity for strong interaction is utilized in their construction. Then these sub¬systems will be linked by the weaker second-order bonds into larger systems. For example, a water molecule has essentially a valence of zero--all the potential covalent bonds are fully occupied by the interaction of hydrogen and oxygen molecules. But the geometry of the molecule creates an elec¬tric dipole that permits weak interaction between the water and salts dissolved in it--whence such phenomena as its electrolytic conductivity."
Similarly, it has been observed that, although electrical forces are much stronger than gravita¬tional forces, the latter are far more important than the former for systems on an astronomical scale. The explanation, of course, is that the elec¬trical forces, being bipolar, are all "used up" in the linkages of the smaller subsystems, and that sig¬nificant net balances of positive or negative charges are not generally found in regions of macroscopic size.
In social as in physical systems there are gen¬erally limits on the simultaneous interaction of large numbers of subsystems. In the social case, these limits are related to the fact that a human being is more nearly a serial than a parallel in¬formation-processing system. He can carry on only one conversation at a time, and although this does not limit the size of the audience to which a mass communication can be addressed, it does
10 See, for example, L. Pauling, General chemistry, ch. 15.
VOL. 106, NO. 6, 19621	THE ARCHITECTURE OF COMPLEXITY	477
limit the number of people simultaneously involved in most other forms of social interaction. Apart from requirements of direct interaction, most roles impose tasks and responsibilities that are time con¬suming. One cannot, for example, enact the role of "friend" with large numbers of other people.
It is probably true that in social as in physical systems, the higher frequency dynamics are associ¬ated with the subsystems, the lower frequency dy¬namics with the larger systems. It is generally believed, for example, that the relevant planning horizon of executives is longer the higher their location in the organizational hierarchy. It is probably also true that both the average duration of an interaction between executives and the aver¬age interval between interactions is greater at higher than at lower levels.
SUMMARY: NEAR DECOMPOSABILITY
We have seen that hierarchies have the property of near-decomposability. Intra-component link¬ages are generally stronger than intercomponent linkages. This fact has the effect of separating the high-frequency dynamics of a hierarchy--in¬volving the internal structure of the components-- from the low frequency dynamics--involving inter¬action among components. We shall turn next to some important consequences of this separation for the description and comprehension of complex systems.
THE DESCRIPTION OF COMPLEXITY
If you ask a person to draw a complex object¬e.g., a human face--he will almost always proceed in a hierarchic fashion.2° First he will outline the face. Then he will add or insert features: eyes, nose, mouth, ears, hair. If asked to elabo¬rate, he will begin to develop details for each of the features--pupils, eyelids, lashes for the eyes, and so on--until he reaches the limits of his ana¬tomical knowledge. His information about the object is arranged hierarchicly in memory, like a topical outline.
When information is put in outline form, it is easy to include information about the relations among the major parts and information about the internal relations of parts in each of the subout¬lines. Detailed information about the relations of subparts belonging to different parts has no place
"George A. Miller has collected protocols from sub¬jects who were given the task of drawing faces, and finds that they behave in the manner described here (private communication). See also E. H. Gombrich, Art and illusion, 291-296, New York, Pantheon Books, 1960. in the outline and is likely to be lost. The loss of such information and the preservation. mainly of information about hierarchic order is a salient characteristic that distinguishes the drawings of a child or someone untrained in representation from the drawing of a trained artist. (I am speaking of an artist who is striving for representation.)
NEAR DECOMPOSABILITY AND COMPREHENSIBILITY
From our discussion of the dynamic properties of nearly decomposable systems, we have seen that comparatively little information is lost by repre¬senting them as hierarchies. Subparts belonging to different parts only interact in an aggregative fashion--the detail of their interaction can be ig¬nored. In studying the interaction of two large molecules, generally we do • not need to consider in detail the interactions of nuclei of the atoms belonging to the one molecule with the nuclei of the atoms belonging to the other. In studying the interaction of two nations, we do not need to study in detail the interactions of each citizen of the first with each citizen of the second.
The fact, then, that many complex systems have a nearly decomposable, hierarchic structure is a major facilitating factor enabling us to under¬stand, to describe, and even to "see" such systems and their parts. Or perhaps the proposition should be put the other way round. If there are important systems in the world that are complex without being hierarchic, they may to a consider¬able extent escape our observation and our under¬standing. Analysis of their behavior would in¬volve such detailed knowledge and calculation of the interactions of their elementary parts that it would be beyond our capacities of memory or computation.21
21 I believe the fallacy in the central thesis of W. M. Elsasser's The physical foundation of biology, mentioned earlier, lies in his ignoring the simplification in description of complex systems that derives from their hierarchic structure. Thus (p. 155): "If we now apply similar arguments to the coupling of enzymatic reactions with the substratum of protein molecules, we see that over a sufficient period of time, the information corresponding to the structural details of these molecules will be commu¬nicated to the dynamics of the cell, to higher levels of organization as it were, and may influence such dynamics. While this reasoning is only qualitative, it lends credence to the assumption that in the living organism, unlike the inorganic crystal, the effects of microscopic structure can¬not be simply averaged out; as time goes on this influ¬ence will pervade the behavior of the cell 'at all levels."
But from our discussion of near-decomposability it would appear that those aspects of microstructure that control the slow developmental aspects of organismic
478	HERBERT A. SIMON	!PROC. AMER. PHIL. SOC.
I shall not try to settle which is chicken and which is egg : whether we are able to understand the world because it is hierarchic, or whether it appears hierarchic because those aspects of it which are not elude our understanding and ob¬servation. I have already given some reasons for supposing that the former is at least half the truth--that evolving complexity would tend to be hierarchic--but it may not be the whole truth.
SIMPLE DESCRIPTIONS OF COMPLEX SYSTEMS
One might suppose that the description of a complex system would itself be a complex struc¬ture of symbols--and indeed, it may be just that. But there is no conservation law that requires that the description be as cumbersome as the ob¬ject described. A trivial example will show how a system can be described economically. Suppose the system is a two-dimensional array like this:
ABMNRSHI
CDOPTUJK
MNABHI RS
m	P C DJ KT U
RSHI ABMN TUJICCDOP
HI RSMNAB
JKTUOPCD
Let us call the array AB cD a, the array o M N p m, RS
the array TU r, and the array Inc HI
 h. Let us rh
call the array lam w, and the array 1hr x. Then
I ma I 
the entire array is simply x27 I. While the original structure consisted of 64 symbols, it re¬quires only 35 to write down its description :
s=wx
XW
=AB
a 
CD	w =am
ma
MN
m=
OP	rh
X=hr
RS TU	HI

We achieve the abbreviation by making use of the redundancy in the original structure. Since
dynamics can be separated out from the aspects that con¬trol the more rapid cellular metabolic processes. For this reason we should not despair of unravelling the web of causes. See also J. R. Platt's review of Elsasser's book in Perspectives in biology and medicine 2: 243-245, 1959.
the pattern AB ' for example, occurs four times CD
in the total pattern, it is economical to represent it by the single symbol, a.
If a complex structure is completely unre¬dundant--if no aspect of its structure can be in¬ferred from any other--then it is its on simplest description. We can exhibit it, but we cannot describe it by a simpler structure. The hierarchic structures we have been discussing have a high degree of redundancy, hence can often be described in economical terms. The redundancy takes a number of forms, of which I shall mention three :
1.	Hierarchic systems are usually composed of only a few different kinds of subsystems, in vari¬ous combinations and arrangements. A familiar example is the proteins, their multitudinous vari¬ety arising from arrangements of only twenty different amino acids. Similarly, the ninety-odd elements provide all the kinds of building blocks needed for an infinite variety of molecules. Hence, we can construct our description from a restricted alphabet of elementary terms corresponding to the basic set of elementary subsystems from which the complex system is generated.
2.	Hierarchic systems are, as we have seen, often nearly decomposable. Hence only aggre¬gative properties of their parts enter into the de¬scription of the interactions of those parts.. A generalization of the notion of near-decomposabil¬ity might be called the "empty world hypothesis" --most things are only weakly connected with most other things; for a tolerable description of reality only a tiny fraction of all possible interac¬tions needs to be taken into account. By adopting a descriptive language that allows the absence of something to go unmentioned, a nearly empty world can be described quite concisely. Mother Hubbard did not have to check off the list of pos¬sible contents to say that her cupboard was bare.
3. By appropriate "recoding." the redundancy that is present but unobvious in the structure of a complex system can often be made patent. The most common recoding of descriptions of dy¬namic systems consists in replacing a description of the time path with a description of a differential law that generates that path. The simplicity, that is, resides in a constant relation between the state of the system at any given time and the state of the system a short time later. Thus, the struc¬ttire of the sequence, 1 3 5 7 9 11 . . ., is most simply expressed by observing that each member is obtained by adding 2 to the previous one. But
VOL. 106, NO. 6, 19621	THE ARCHITECTURE OF COMPLEXITY	479
this is the sequence that Galileo found to describe the velocity at the end of successive time intervals of a ball rolling down an inclined plane.
It is a familiar proposition that the task of sci¬ence is to make use of the world's redundancy to describe that world simply. I shall not pursue the general methodological point here, but shall instead take a closer look at two main types of description that seem to be available to us in seeking an understanding. of complex systems. I shall call these state description and process description, respectively.
STATE DESCRIPTIONS AND PROCESS DESCRIPTIONS
"A circle is the locus of all points equidistant from a given point." "To construct a circle, rotate a compass with one arm fixed until the other arm has returned to its starting point." It is implicit in Euclid that if you carry out the process speci¬fied in the second sentence, you will produce an object that satisfies the definition of the first. The first sentence is a state description of a circle, the second a process description.
These two modes of apprehending structure are the warp and weft of our experience. Pictures, blueprints, most diagrams, chemical structural formulae are state descriptions. Recipes, differ¬ential equations, equations for chemical reactions are process descriptions. The former characterize the world as sensed ; they provide the criteria for identifying objects, often by modeling the objects themselves. The latter characterize the world as acted upon ; they provide the means for producing or generating objects having the desired charac¬teristics.
The distinction between the world as sensed and the world as acted upon defines the basic condition for the survival of adaptive organisms. The or¬ganism must develop correlations between goals in the sensed world and actions in the world of process. When they are made conscious and ver¬balized, these correlations correspond to what we usually call means-end analysis. Given a desired state of affairs and an existing state of affairs, the task of an adaptive organism is to find the differ¬ence between these two states, and then to find the correlating process that will erase the differ¬ence.22
Thus, problem solving requires continual trans 
== See H. A. Simon and A. Newell, Simulation of human thinking, in M. Greenberger (ed.), Management
and the computer of the future, 95-114, esp. pp 110 ff.,
New York, Wiley, 1962. lation between the state and process descriptions of the same complex reality. Plato, in the Meno, argited that all learning is remembering. He could not otherwise explain how we can discover or recognize the answer to a problem unless we already know the answer." Our dual relation to the world is the source and solution of the para¬dox. -We pose a problem by giving the state description of the solution. The task is to dis¬cover a sequence of processes that will produce the goal state from an initial state. Translation from the process description to the state descrip¬tion enables us to recognize when we have suc¬ceeded. The solution is genuinely new to us-- and we do not need Plato's theory of remembering to explain how we recognize it.
There is now a growing body of evidence that the activity called human problem solving is basi¬cally a form of means-end analysis that aims at discovering a process description of the path that leads to a desired goal. The general paradigm is : given a blueprint, to find the corresponding recipe. Much of the activity of science is an application of that paradigm : given the descrip¬tion of some natural phenomena, to find the differ¬ential equations for processes that will produce the phenomena.
THE DESCRIPTION OF COMPLEXITY IN
SELF-REPRODUCING SYSTEMS
The problem of finding relatively simple descrip¬tions for complex systems is of interest not only for an understanding of human knowledge of the world but also for an explanation of how a com¬plex system can reproduce itself. In my discus¬sion of the evolution of complex systems, I touched only briefly on the role of self-reproduction.
Atoms of high atomic weight and complex in¬organic molecules are witnesses to the fact that the evolution of complexity does not imply self- reproduction. If evolution of complexity from simplicity is sufficiently probable, it will occur repeatedly ; the statistical equilibrium of the system will find a large fraction of the elementary par¬ticles participating in complex systems.
If, however, the existence of a particular com¬plex form increased the probability of the creation of another form just like it, the equilibrium be¬tween complexes and components could be greatly altered in favor of the former.' If we have a de¬scription of an object that is sufficiently clear and
23 The works of Plato, B. Jowett, trans., 3: 26-35, New York, Dial Press.

480	HERBERT A. SIMON	(PROC. AMER. PHIL. SOC.

complete, we can reproduce the object from the description. Whatever the exact mechanism of reproduction, the description provides us with the necessary information.
Now we have seen that the descriptions of com¬plex systems can take many forms. In particular, we can have state descriptions or we can have process descriptions ; blueprints or recipes. Re¬productive processes could be built around either of these sources of information. Perhaps the simplest possibility is for the complex system to serve as a description of itself--a template on which a copy can be formed. One of the most plausible current theories, for example, of the reproduction of deoxyribonucleic acid (DNA) proposes that a DNA molecule, in the form of a double helix of matching parts (each essentially a "negative" of the other), unwinds to allow each half of the helix to serve as a template on which a new matching half can form.
On the other hand, our current knowledge of how DNA controls the metabolism of the organ¬ism suggests that reproduction by template is only one of the processes involved. According to the prevailing theory, DNA serves as a template both for itself and for the related substance ribonucleic acid (RNA). RNA, in turn, serves as a template for protein. But proteins--according to current knowledge--guide the organism's metabolism not by the template method but by serving as catalysts to govern reaction rates in the cell. While RNA is a blueprint for protein, protein is a recipe for metabolism."
ONTOGENY RECAPITULATES PHYLOGENY
The DNA in the chromosomes of an organism contains some, and perhaps most, of the informa¬tion that is needed to determine its development and activity. We have seen that, if current theo¬ries are even approximately correct, the informa¬tion is recorded not as a state description of the organism but as a series of "instructions" for the construction and maintenance of the organism from nutrient materials. I have already used the metaphor of a _recipe ; I could equally well compare it with a computer program, which is also a se¬quence of instructions, governing the construction
24 C. B. Anfinsen, The molecular basis of evolution, chs. 3 and 10, New York, Wiley, 1959, will qualify this sketchy, oversimplified account. For an imaginative dis¬cussion of some mechanisms of process description that could govern molecular structure, see H. H. Pattee, On the origin of macromolecular sequences, Biophysical Jour¬nal 1: 683-710, 1961. of syMbolic structures. Let me spin out some of the consequences of the latter comparison.
'If genetic material is a program--viewed in its relation to the organism--it is a program with special and peculiar properties. First, it is a self- reproducing program ; we have already considered its possible copying mechanism. Second, it is a program that has developed by Darwinian evolu¬tion. On the basis of our watchmaker's argument, we may assert that many of its ancestors were also viable programs--programs for the subassemblies.
Are there any other conjectures we can make about the structure of this program? There is a well-known generalization in biology that is ver¬bally so neat that we would be reluctant to give it up even if the facts did not support it : ontogeny recapitulates phylogeny. The individual organism, in its development, goes through stages that re¬semble some of its ancestral forms. The fact that the human embryo develops gill bars and then modifies them for other purposes is a familiar particular belonging to the generalization. Biolo¬gists today like to emphasize the qualifications of the principle--that ontogeny recapitulates only the grossest aspects of phylogeny, and these only crudely. These qualifications should not make us lose sight of the fact that the generalization does hold in rough approximation--it does summarize a very significant set of facts about the organism's development. How can we interpret these facts?
One way to solve a complex problem is to reduce it to a problem previously solved--to show what steps lead from the earlier solution to a solu¬tion of the new problem. If, around the turn of the century, we wanted to instruct a workman to make an automobile, perhaps the simplest way would have been to tell him how to modify a wagon by removing the singletree and adding a motor and transmission. Similarly, a genetic program could be altered in the course of evolu¬tion by adding new processes that would modify a simpler form into a more complex one--to con¬struct a gastrula, take a blastula and alter it!
The genetic description of a single cell may, therefore, take a quite different form from the genetic description that assembles cells into a multi-celled organism. Multiplication by cell divi¬sion would require, as a minimum, a state descrip¬tion (the DNA, say), and a simple "interpretive process"--to use the term from computer language --that copies this description as a part of the larger copying process of cell division. But such a mechanism clearly would not suffice for the
VOL. 106, NO. 6, 1962)	THE ARCHITECTURE OF COMPLEXITY	481
differentiation of cells in development. It appears more natural to conceptualize that mechanism as based on a process description, and a somewhat more complex interpretive process that produces the adult organism in a sequence of stages, each new stage in development representing the effect of an operator upon the previous one.
It is harder to conceptualize the interrelation of these two descriptions. Interrelated they must be, for, enough has been learned of gene-enzyme mechanisms to show that these play a major role in development as in cell metabolism. The single clue we obtain from our earlier discussion is that the description may itself be hierarchical, or nearly decomposable, in structure, the lower levels gov¬erning the fast, "high-frequency" dynamics of the individual cell, the higher level interactions gov¬erning the slow, "low-frequency" dynamics of the developing multi-cellular organism.
There are only bits of evidence, apart from the facts of recapitulation, that the genetic program is organized in this way, but such evidence as exists is compatible with this notion.25 To the extent that we can differentiate the genetic information that governs cell metabolism from the genetic in¬formation that governs the development of differ¬entiated cells in the multi-cellular organization, we simplify enormously--as we have already seen --our task of theoretical description. But I have perhaps pressed this speculation far enough.
The generalization that in evolving systems whose descriptions are stored in a process lan¬guage, we might expect ontogeny partially to re¬capitulate phylogeny has applications outside the
25 There is considerable evidence that successive genes along a chromosome often determine enzymes controlling successive stages of protein syntheses. For a review of some of this evidence, see P. E. Hartman, Transduction: a comparative review, in W. D. McElroy and B. Glass (eds.), The chemical basis of heredity, Baltimore, Johns Hopkins Press, 1957, at pp. 442-454. Evidence for dif¬ferential activity of genes in different tissues and at differ¬ent stages of development is discussed by J. G. Gall, Chromosomal Differentiation, in W. D. McElroy and B. Glass (eds.), The chemical basis of development, Baltimore, Johns Hopkins Press, 1958, at pp. 103-135. Finally, a model very like that proposed here has been independently, and far more fully, outlined by J. R. Platt, A 'book model' of genetic information transfer in cells and tissues, in Kasha and Pullman (eds.), Horizons in biochemistry, New York, Academic Press, forthcoming. Of course, this kind of mechanism is not the only one in which development could be controlled by a process de¬scription. Induction, in the form envisaged in Spemann's organizer theory, is based on process description, in which metabolites in already formed tissue control the next stages of development. realm of biology. It can be applied as readily, for example, to the transmission of knowledge in the 'educational process. In most subjects, par¬ticularly in the rapidly advancing sciences, the progress from elementary to advanced courses is to a considerable extent a progress through the conceptual history of the science itself. Fortu¬nately, the recapitulation is seldom literal--any more than it is in the biological case. We do not teach the phlogiston theory in chemistry in order later to correct it. (I am not sure I could not cite examples in other subjects where we do ex¬actly that.) But curriculum revisions that rid us of the accumulations of the past are infrequent and painful. Nor are they always desirable--par¬tial recapitulation may, in many instances, provide the most expeditious route to advanced knowledge.
sum ARY : THE DESCRIPTION OF COMPLEXITY
How complex or simple a structure is depends critically upon the way in which we describe it. Most of the complex structures found in the world are enormously redundant, and we can use this redundancy to simplify their description. But to use it, to achieve the simplification, we must find the right representation.
The notion of substituting a process description for a state description of nature has played a cen¬tral role in the development of modern science. Dynamic laws, expressed in the form of systems of differential or difference equations, have in a large number of cases provided the clue for the simple description of the complex. In the pre¬ceding paragraphs I have tried to show that this characteristic of scientific inquiry is not accidental or superficial. The correlation between state de¬scription and process description is basic to the functioning of any adaptive organism, to its ca¬pacity for acting purposefully upon its environ¬ment. Our present-day understanding of genetic mechanisms suggests that even in describing itself the multi-cellular organism finds a process descrip¬tion--a genetically encoded program--to be the parsimonious and useful representation.
CONCLUSION
Our speculations have carried us over a rather alarming array of topics, but that is the price we must pay if we wish to seek properties common to many sorts of complex systems. My thesis has been that one path to the construction of a non- -trivial theory of complex systems is by way of a theory of hierarchy. Empirically, a large propor¬tion of the complex systems we observe in nature

482	HERBERT A. SIMON	[PROC. AMER. PHIL. SOC.

exhibit hierarchic structure. On theoretical
grounds we could expect complex systems to be . hierarchies in a world in which complexity had to evolve from simplicity. In their dynamics, hier¬archies have a property, near-decomposability, that greatly simplifies their behavior. Near-decom¬posability also simplifies the description of a com¬plex system, and makes it easier to understand how the information needed for the development or reproduction of the system can be stored in reasonable compass.
In both science and engineering, the study of "systems" is an increasingly popular activity. Its popularity is more a response to a pressing need for synthesizing and analyzing complexity than it is to any large development of a body of knowl¬edge and technique for dealing with complexity. If this popularity is to :)e more than a fad, neces¬sity will have to mother invention and provide substance to go with tl-e name. The explorations reviewed here represe It one particular direction of search for such substance.

